{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw_les_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_Q__5kdYGzj"
      },
      "source": [
        "## Практическое задание\n",
        "\n",
        "Вариант 1. (простой)\n",
        "\n",
        "- обучить сверточную нейронную сеть в стиле AlexNet (с падением размера ядра свертки и последовательностью блоков свертка-пулинг  (conv-pool)-(conv-pool)-...) на датасете fashion-mnist\n",
        "- оценить рост точности при увеличении ширины сети (больше ядер)\n",
        "- оценить рост точности при увеличении глубины сети (больше слоев)\n",
        "- сравнить с точностью полносвязной сети для этой выборки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7STI8LNjXsYJ"
      },
      "source": [
        "from __future__ import print_function\n",
        "#import tensorflow.keras as keras # расскоментируйте эту строку, чтобы начать обучение\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTcf1Do17KsP"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "pZO6uOVwR8Xw",
        "outputId": "eee25c0f-e7fc-4501-f961-c923c360ab8f"
      },
      "source": [
        "plt.imshow(x_train[1501,:,:])\n",
        "plt.show()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i052vXT4Y_B1"
      },
      "source": [
        "param_dict = dict()"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "274iCWkhXI29",
        "outputId": "557e02dd-ed06-453d-d54e-2cfed57f5cfd"
      },
      "source": [
        "param_dict.update({'num_pixels': len(x_train[1501][0])})\n",
        "print(f'Количество пикселей { param_dict[\"num_pixels\"] }')"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Количество пикселей 28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZLheRwKTISP",
        "outputId": "713b7c51-8bde-4c69-9115-9efcf40701df"
      },
      "source": [
        "# Кол-во классов\n",
        "classes = set(y_train)\n",
        "param_dict['num_classes'] = len(classes)\n",
        "print(f'Кол-во классов {param_dict[\"num_classes\"]} - {classes}')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Кол-во классов 10 - {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvU6C3HCSaF4",
        "outputId": "356badc3-2712-4586-ef85-0d388deb9153"
      },
      "source": [
        "# Normalize the images.\n",
        "x_train = (x_train / 127) - 1\n",
        "x_test = (x_test / 127) - 1\n",
        "\n",
        "# Flatten the images.\n",
        "x_train = x_train.reshape((-1, 784))\n",
        "x_test = x_test.reshape((-1, 784))\n",
        "\n",
        "print(x_train.shape) # (60000, 784)\n",
        "print(x_test.shape)  # (10000, 784)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUNPVQ8qVDGP"
      },
      "source": [
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNxCOVVcY5X5"
      },
      "source": [
        "param_dict['batch_size'] = 32\n",
        "param_dict['num_classes']  = 10\n",
        "param_dict['epochs']  = 5\n",
        "param_dict['data_augmentation']  = False\n",
        "param_dict['num_predictions']  = 20\n",
        "\n",
        "param_dict['x_train'] = x_train \n",
        "param_dict['x_test'] = x_test\n",
        "\n",
        "param_dict['y_train'] = y_train\n",
        "param_dict['y_test'] = y_test"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFaFw_y_Xxeg"
      },
      "source": [
        "def create_fit_model(param_dict):\n",
        "  # конфигурирование слоев нейросети\n",
        "  model = Sequential()\n",
        "\n",
        "  # слои нейросети отвественные за свертку и max-pooling\n",
        "  # model.add(Conv2D(param_dict['num_pixels'], (3, 3), padding='same',\n",
        "  #                 input_shape=x_train.shape[1:]))\n",
        "  model.add(layers.experimental.preprocessing.Resizing(224, 224, interpolation=\"bilinear\", input_shape=param_dict['x_train'].shape[1:]))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(param_dict['num_pixels'], (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout( 0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  # полносвязные слои нейронной сети\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  # инициализация RMSprop optimizer\n",
        "  #opt = tensorflow.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "  # компиляция модели\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='SGD',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  if not param_dict['data_augmentation']:\n",
        "      print('Не используется data augmentation')\n",
        "      model.fit(param_dict['x_train'], param_dict['y_train'],\n",
        "                batch_size=param_dict['batch_size'],\n",
        "                epochs=param_dict['epochs'],\n",
        "                validation_data=(param_dict['x_test'], \n",
        "                                 param_dict['y_test']),\n",
        "                shuffle=True)\n",
        "  else:\n",
        "      print('Использование data augmentation в реальном времени')\n",
        "      # Препроцессинг и data augmentation в реальном времени:\n",
        "      datagen = ImageDataGenerator(\n",
        "          featurewise_center=False,\n",
        "          samplewise_center=False,\n",
        "          featurewise_std_normalization=False,\n",
        "          samplewise_std_normalization=False,\n",
        "          zca_whitening=False, \n",
        "          zca_epsilon=1e-06, \n",
        "          rotation_range=0, \n",
        "          width_shift_range=0.1,\n",
        "          height_shift_range=0.1,\n",
        "          shear_range=0., \n",
        "          zoom_range=0., \n",
        "          channel_shift_range=0.,\n",
        "          fill_mode='nearest',\n",
        "          cval=0.,\n",
        "          horizontal_flip=True,\n",
        "          vertical_flip=False,\n",
        "          rescale=None,\n",
        "          preprocessing_function=None,\n",
        "          data_format=None,\n",
        "          validation_split=0.0)\n",
        "\n",
        "      # запуск data augmentation через fit\n",
        "      #datagen.fit(x_train)\n",
        "\n",
        "      # запуск data augmentation через fit_generator\n",
        "      model.fit_generator(datagen.flow(\n",
        "          param_dict['x_train'], \n",
        "          param_dict['y_train'],\n",
        "          batch_size=param_dict['batch_size']),\n",
        "                          epochs=param_dict['epochs'],\n",
        "                          validation_data=(param_dict['x_test'], \n",
        "                                           param_dict['y_test']),\n",
        "                          workers=4)\n",
        "  # проверка работы обученной модели\n",
        "  scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "  print('Test loss:', scores[0])\n",
        "  print('Test accuracy:', scores[1])\n",
        "    "
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "-beLKdyyZ9yJ",
        "outputId": "9399b6de-ba21-4eb4-b436-e3382b0a367c"
      },
      "source": [
        "create_fit_model(param_dict)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-0af84f480f2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-100-1debc9a78c4a>\u001b[0m in \u001b[0;36mcreate_fit_model\u001b[0;34m(param_dict)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# model.add(Conv2D(param_dict['num_pixels'], (3, 3), padding='same',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m#                 input_shape=x_train.shape[1:]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bilinear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_pixels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    211\u001b[0m           \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m           \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m           \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m           \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 970\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1106\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1108\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_width\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         method=self._interpolation_method)\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36mresize_images_v2\u001b[0;34m(images, size, method, preserve_aspect_ratio, antialias, name)\u001b[0m\n\u001b[1;32m   1721\u001b[0m       \u001b[0mpreserve_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreserve_aspect_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m       skip_resize_if_same=False)\n\u001b[0m\u001b[1;32m   1724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36m_resize_images_common\u001b[0;34m(images, resizer_fn, size, preserve_aspect_ratio, name, skip_resize_if_same)\u001b[0m\n\u001b[1;32m   1394\u001b[0m       \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1396\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\'images\\' must have either 3 or 4 dimensions.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'images' must have either 3 or 4 dimensions."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ILKKlVbPyM"
      },
      "source": [
        "  # конфигурирование слоев нейросети\n",
        "  model = Sequential()"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "DIV0l8i9dXuU",
        "outputId": "4eb692fa-f840-4a8f-bf1f-6e9818e17817"
      },
      "source": [
        "model.add(Conv2D(param_dict['num_pixels'], (3, 3), padding='same',\n",
        "                 input_shape=param_dict['x_train'].shape[1:]))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-130cf498db9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.add(Conv2D(param_dict['num_pixels'], (3, 3), padding='same',\n\u001b[0;32m----> 2\u001b[0;31m                  input_shape=param_dict['x_train'].shape[1:]))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    211\u001b[0m           \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m           \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m           \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m           \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 970\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1106\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1108\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    876\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2598\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[0;32m-> 2600\u001b[0;31m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   2601\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2602\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m                          \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                          \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                          str(tuple(shape)))\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;31m# Check dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer conv2d_6 is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: (None, 784)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQsS5Q-5bScX"
      },
      "source": [
        "# слои нейросети отвественные за свертку и max-pooling\n",
        "from tensorflow.keras import models, layers, losses, datasets\n",
        "model.add(layers.experimental.preprocessing.Resizing(224, 224, interpolation=\"bilinear\", input_shape=x_train.shape[1:]))"
      ],
      "execution_count": 84,
      "outputs": []
    }
  ]
}